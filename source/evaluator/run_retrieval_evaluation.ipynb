{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74fd7b35",
   "metadata": {},
   "source": [
    "### Run\n",
    "Only run retriever evaluator on Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ace5a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path(os.getcwd()).parent.parent))\n",
    "\n",
    "from source.rag.retrieval import RetrievalPipeline\n",
    "from llama_index.core.evaluation import RetrieverEvaluator\n",
    "from llama_index.core.evaluation import (\n",
    "    EmbeddingQAFinetuneDataset,\n",
    ")\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aeaaa648",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| f'Collection {self.index_name} have existed': 'Collection trduy_test2 have existed'\n",
      "/workspace/competitions/Sly/Duy_NCKH_2025/source/database/qdrant.py:51: UserWarning: Qdrant client version 1.14.2 is incompatible with server version 1.11.3. Major versions should match and minor version difference must not exceed 1. Set check_compatibility=False to skip version check.\n",
      "  self.client = QdrantClient(url, timeout=60)\n",
      "ic| Settings: _Settings(_llm=OpenAI(callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x7fcc1c8b0d30>, system_prompt=None, messages_to_prompt=<function messages_to_prompt at 0x7fcc22d82320>, completion_to_prompt=<function default_completion_to_prompt at 0x7fcc22c1a3b0>, output_parser=None, pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'>, query_wrapper_prompt=None, model='gpt-4o-mini', temperature=0.1, max_tokens=None, logprobs=None, top_logprobs=0, additional_kwargs={}, max_retries=3, timeout=60.0, default_headers={}, reuse_client=True, api_key='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', api_base='https://api.openai.com/v1', api_version='', strict=False),\n",
      "                        _embed_model=OpenAIEmbedding(model_name='text-embedding-3-large', embed_batch_size=100, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x7fcc1c8b0d30>, num_workers=None, additional_kwargs={}, api_key='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', api_base='https://api.openai.com/v1', api_version='', max_retries=10, timeout=60.0, default_headers=None, reuse_client=True, dimensions=None),\n",
      "                        _callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x7fcc1c8b0d30>,\n",
      "                        _tokenizer=None,\n",
      "                        _node_parser=SentenceSplitter(include_metadata=True, include_prev_next_rel=True, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x7fcc1c8b0d30>, id_func=<function default_id_func at 0x7fcc22c5f5b0>, chunk_size=1024, chunk_overlap=200, separator=' ', paragraph_separator='\n",
      "              \n",
      "              \n",
      "              ', secondary_chunking_regex='[^,.;。？！]+[,.;。？！]?'),\n",
      "                        _prompt_helper=None,\n",
      "                        _transformations=[SentenceSplitter(include_metadata=True, include_prev_next_rel=True, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x7fcc1c8b0d30>, id_func=<function default_id_func at 0x7fcc22c5f5b0>, chunk_size=1024, chunk_overlap=200, separator=' ', paragraph_separator='\n",
      "              \n",
      "              \n",
      "              ', secondary_chunking_regex='[^,.;。？！]+[,.;。？！]?')])\n"
     ]
    }
   ],
   "source": [
    "metrics = ['mrr']\n",
    "retriever = RetrievalPipeline()\n",
    "\n",
    "retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
    "    metrics, retriever=retriever, workers=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60639161",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_dataset = EmbeddingQAFinetuneDataset.from_json(\"/workspace/competitions/Sly/Duy_NCKH_2025/data/retrieval_evaluation/qa_dataset2.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2d43b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Nếu tôi điều khiển xe chở hàng vượt trọng tải cho phép từ 100% đến 150%, tôi sẽ bị phạt bao nhiêu tiền?\n",
      "Metrics: {'mrr': 1.0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# try it out on a sample query\n",
    "sample_id, sample_query = list(qa_dataset.queries.items())[2]\n",
    "sample_expected = qa_dataset.relevant_docs[sample_id]\n",
    "\n",
    "eval_result = retriever_evaluator.evaluate(sample_query, sample_expected)\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19455656",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/628 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 628/628 [43:19<00:00,  4.14s/it]    \n"
     ]
    }
   ],
   "source": [
    "# try it out on an entire dataset\n",
    "eval_results = await retriever_evaluator.aevaluate_dataset(qa_dataset, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "157f7da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_eval_results_to_json(eval_results, output_path: str):\n",
    "    simplified_results = []\n",
    "\n",
    "    for result in eval_results:\n",
    "        simplified = {\n",
    "            \"query\": result.query,\n",
    "            \"expected_ids\": result.expected_ids,\n",
    "            \"expected_texts\": result.expected_texts,\n",
    "            \"retrieved_ids\": result.retrieved_ids,\n",
    "            \"retrieved_texts\": result.retrieved_texts,\n",
    "            \"score\": result.metric_dict[\"mrr\"].score # Lấy điểm MRR nếu có\n",
    "        }\n",
    "        simplified_results.append(simplified)\n",
    "\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(simplified_results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "output_path = '/workspace/competitions/Sly/Duy_NCKH_2025/data/retrieval_evaluation/experiment_2/qa_result2.json'\n",
    "save_eval_results_to_json(eval_results, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfbcaf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def display_results(name, eval_results):\n",
    "    \"\"\"Display results from evaluate.\"\"\"\n",
    "\n",
    "    metric_dicts = []\n",
    "    for eval_result in eval_results:\n",
    "        metric_dict = eval_result.metric_vals_dict\n",
    "        metric_dicts.append(metric_dict)\n",
    "\n",
    "    full_df = pd.DataFrame(metric_dicts)\n",
    "\n",
    "    columns = {\n",
    "        \"retrievers\": [name],\n",
    "        **{k: [full_df[k].mean()] for k in metrics},\n",
    "    }\n",
    "\n",
    "    metric_df = pd.DataFrame(columns)\n",
    "\n",
    "    return metric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79d87ffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retrievers</th>\n",
       "      <th>mrr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>with no reranking</td>\n",
       "      <td>0.601166</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          retrievers       mrr\n",
       "0  with no reranking  0.601166"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_results(\"with no reranking\", eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6dea9e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Nếu tôi điều khiển xe mà không có giấy chứng nhận kiểm định an toàn kỹ thuật còn hiệu lực, tôi sẽ bị phạt bao nhiêu tiền và có thể bị xử lý như thế nào?\n",
      "Metrics: {'mrr': 0.2}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(eval_results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b028153a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "traffic_law",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
